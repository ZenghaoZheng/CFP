# 基于策略的强化学习
传统策略梯度算法——>（引入KL散度）自然策略梯度算法->（引入共轭梯度法+线搜索+改进检查）TRPO->（引入clip函数）PPO
## 1. 策略梯度算法
### 基于值函数的强化学习
这些算法在学习后的Q值函数不再发生变化，每次做出的策略也是一定的，可以理解为确定性策略。每次选择对应Q值最大的动作

基于策略的强化学习不再通过价值函数来确定选择动作的策略，而是直接学习策略本身，通过一组参数θ对策略进行参数化，并通过神经网络方法优化θ。
定义目标函数：
![image-20250420100524812](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420100524812.png)

$\tau$是动作轨迹，定义在$\theta$条件下轨迹为：

$s_0->a_0->s_1->a_1->s_2->......$

![image-20250420101110104](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420101110104.png)

这里可能少写一个$s_0$，代表初始状态，一般可以为1.

由于需要优化神经网络，计算$J(\theta)$的梯度：

![image-20250420101303648](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420101303648.png)

将$P(\tau;\theta)$带入上述公式中

![image-20250420101920027](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420101920027.png)

![image-20250420102031965](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420102031965.png)

使用Monte-Carlo近似数学期望，并将化简后的$\nabla_\theta logP(\tau;\theta)$带入$J_\theta$的梯度，化简得到上述公式

这里$R(\tau^{(i)})$虽然包含了轨迹$\tau$，但是也可以整体计算，<u>（是不是也可以理解为nlp任务中 直接对整体的奖励，针对每个轨迹，不需要分布计算奖励）</u>，将$R(\tau^{(i)})$等于及时奖励的衰减累积$ \sum_{t=0}^T \gamma^{t}r_t $，可以看作与前面的求导无关。忽略时间步

但是更加常用的做法是，将$R(\tau^{(i)})$分解为每一步的奖励，那么$R(\tau^{(i)})$是一个与t有关的函数，将其与前面对数概率公式合并，得到针对每一个时间步的公式：
![image-20250420110038445](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420110038445.png)

现在这个梯度是可以计算的，使用梯度上升法更新公式：![image-20250420110319460](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420110319460.png)

对于离散动作空间，多采用softmax进行计算，对应于nlp任务每个token输出的概率，对于连续动作空间，使用高斯策略。

那么一个常用的REINFORCE 算法工作流程如下：

![image-20250420111005321](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420111005321.png)

```python
class REINFORCE:
    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,
                 device):
        self.policy_net = PolicyNet(state_dim, hidden_dim,
                                    action_dim).to(device)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(),
                                          lr=learning_rate) 
        self.gamma = gamma  # 折扣因子
        self.device = device

    def take_action(self, state):  # 根据动作概率分布随机采样
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.policy_net(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        '''
        transition_dict: 一条完整轨迹
        '''
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']

        G = 0
        self.optimizer.zero_grad()
        for i in reversed(range(len(reward_list))):  # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]],
                                 dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = self.gamma * G + reward
            loss = -log_prob * G  # 每一步的损失函数
            loss.backward()  # 叠加每个时间步反向传播计算梯度
        self.optimizer.step()  # 梯度下降
```

## 2. 自然策略梯度算法

自然策略梯度算法揭露了传统策略梯度算法的缺点以及补救的方法

### 传统策略梯度算法的缺陷

主要通过梯度上升的方法来最大化这个目标函数，使得策略最优。但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。

现存的深度学习模型比如MLP也很大程度上出现过陷入局部最优或者幅度过大的问题，但是配合下降学习率和Adam等优化器已经可以减少这种情况，同时数据集是不变的，就算出现这种情况也可以即使改正。

但是RL方法当前的步长会影响后续的采样过程，引起连锁反应，输入的数据集是变化的，所以很大可能回不来。

### 自然策略梯度算法

考虑在更新时找到一块**信任区域**（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是**信任区域策略优化**（trust region policy optimization，TRPO）算法的主要思想。

置信域算法是数值优化的内容，对于一个复杂难以求导的函数$J_ \theta$，在$\theta$的邻域内找到一个较为简单的函数$L_\theta$来近似$J_ \theta$，这样可以在邻域内将最大化$J_ \theta$转化为最大化$L_\theta$来求解，迭代这个过程，可以找到$J_ \theta$的全局最大值。

构造$L_\theta$的方法包括：

* $J_ \theta$二阶泰勒展开式
* $J_ \theta$的Monte-Carlo近似

#### 推导目标函数

为了限制每一次参数更新不能离开原始分布太远，计算两种策略分布之间的差异最常用的算法是计算它们之间的KL散度

![image-20250420153205219](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420153205219.png)

将公式代入优化目标函数，也就是最大化新策略下的目标函数，使用拉格朗日松弛将原表达式的发散约束转化为惩罚项![image-20250420153700097](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420153700097.png)

将上述公式在$\theta_{old}$处进行二阶泰勒展开：第一个公式进行一阶泰勒展开，KL散度公式的一阶和二阶展开是0.

![image-20250420154111721](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420154111721.png)

在这里，首先去除与$\Delta \theta$无关的项，然后使用使用Fisher信息矩阵，替换KL散度的二阶导数

![image-20250420154411996](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420154411996.png)

这里Fisher信息矩阵在局部等价于关于old策略的海塞矩阵。

### 优化求解

如果要求目标函数最大值对应的x值，可以令对应的导数为0，求解方程。

根据上式

![image-20250420155330830](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420155330830.png)

这里$\lambda$是一个常数，可以放到学习率$\alpha$中，同时根据KL散度小于小于$\epsilon$可以退出动态学习率$\alpha$，下面的$F(\theta)$和$\nabla J(\theta)$应该在分母

![image-20250420160120887](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420160120887.png)

那么$\nabla^{-} J_\theta$就被称为自然策略梯度，也就是$F(\theta)^{-1} \nabla_{\theta}J(\theta)$。

那么结合学习率和自然策略梯度的公式为：

![image-20250420161914375](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420161914375.png)

### 算法流程

![image-20250420162215641](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420162215641.png)

这里的H矩阵可以使用![image-20250420162718948](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420162718948.png)

也就是原始策略的海塞矩阵代替。

不同于传统策略梯度算法的优点：

* 策略梯度由逆Fisher矩阵校正
* 更新步长 α 具有适应梯度和局部敏感性的动态表达式

## 3. Trust region policy optimization 信赖域策略优化算法（TRPO）

TRPO是以自然策略梯度优化为基础，它在经验上比自然策略梯度算法表现得更好、更稳定。

### 自然策略梯度算法的缺陷：

近似值可能会**违反KL约束**，从而导致分析得出的步长过大，超出限制要求

矩阵$F^{-1}$的计算时间过长

由于存在大量的近似过程，策略可能并没有优化

### 算法设计

我们希望可以对策略的优化进行量化，从而保证每次的更新一定是优化作用的。

采用的原策略预期回报添加新策略预期优势构成新策略的回报

![image-20250420164155938](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420164155938.png)

其中**优势函数A**的定义为：

![image-20250420164249794](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420164249794.png)

在给定的策略和状态下，计算特定动作$a$的期望累积奖励与总体期望值的差值，描述了该动作的相对吸引力。

定义**折扣访问频率**：

![image-20250420170239389](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420170239389.png)

那么就可以将对时间步的求和改写为对状态的求和（这里采用的是别的符号进行求和）

![image-20250420170653174](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420170653174.png)

也就是下式：

![image-20250420170810504](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420170810504.png)

引入更新策略的近似误差，使用当前策略近似：

![image-20250420171402862](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420171402862.png)

接下来，我们将状态分布求和替换为期望，方便实际计算时使用蒙特卡洛模拟进行采样，同时将动作求和替换为**重要性采样**。![image-20250420171803864](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420171803864.png)

（上面的公式应该有问题，需要对原始函数求期望）

下面的公式被称为**替代优势**：

![image-20250420173330150](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420173330150.png)

我们希望后面这一部分尽可能的大，并且最好是大于0，而且要保证KL散度条件

### 改进策略

**共轭梯度法（conjugate gradient method）**，引入共轭梯度法，计算自然策略梯度

**线搜索**， TRPO 通过执行线搜索来解决此问题，通过不断地迭代减小更新的大小，直到第一个不违反约束的更新。这个过程可以看作是不断缩小信任区域，即我们相信更新可以实际改进目标的区域。其实就是指数衰减率。

**改进检查**，在TRPO中，并没有假设更新会提高替代优势 L(θ) ，而是真正检查了它。尽管实际计算时需要根据旧策略计算优势，以及使用重要性抽样来调整概率，会花费一些时间，但验证更新是否真正改进了策略是有必要的。

**广义优势估计**，结合**时序差分误差**估计优势函数$A$的值，在策略梯度算法中的梯度$J_(\theta)$是用轨迹回报的蒙特卡洛模拟来得到的，但是蒙特卡洛模拟的误差较大，这里采用另一种方法。

* 这里还涉及到使用**Actor-Critic算法**更新Critic的过程，定义**时序差分残差**来指导策略梯度进行学习

![image-20250420201944118](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420201944118.png)

定义它的损失函数为![image-20250420202258240](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420202258240.png)

然后更新带有参数的价值网络，它主要用来估计V状态价值函数。

```python
class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)


class ValueNet(torch.nn.Module):	# 评估状态的价值，用于计算优势函数（Advantage）
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)


class TRPO:
    """ TRPO算法 """
    def __init__(self, hidden_dim, state_space, action_space, lmbda,
                 kl_constraint, alpha, critic_lr, gamma, device):
        state_dim = state_space.shape[0]
        action_dim = action_space.n
        # 策略网络参数不需要优化器更新
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)
        self.gamma = gamma
        self.lmbda = lmbda  # GAE参数
        self.kl_constraint = kl_constraint  # KL距离最大限制
        self.alpha = alpha  # 线性搜索参数
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()	# 根据概率选择动作1或2，确保探索与利用的平衡。

    def hessian_matrix_vector_product(self, states, old_action_dists, vector):
        # 计算黑塞矩阵和一个向量的乘积
        new_action_dists = torch.distributions.Categorical(self.actor(states))
        kl = torch.mean(
            torch.distributions.kl.kl_divergence(old_action_dists,
                                                 new_action_dists))  # 计算平均KL距离
        kl_grad = torch.autograd.grad(kl,
                                      self.actor.parameters(),
                                      create_graph=True)
        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])
        # KL距离的梯度先和向量进行点积运算
        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)
        grad2 = torch.autograd.grad(kl_grad_vector_product,
                                    self.actor.parameters())
        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])
        return grad2_vector

    def conjugate_gradient(self, grad, states, old_action_dists):  # 共轭梯度法求解方程
        x = torch.zeros_like(grad)
        r = grad.clone()
        p = grad.clone()
        rdotr = torch.dot(r, r)
        for i in range(10):  # 共轭梯度主循环
            Hp = self.hessian_matrix_vector_product(states, old_action_dists,
                                                    p)
            alpha = rdotr / torch.dot(p, Hp)
            x += alpha * p
            r -= alpha * Hp
            new_rdotr = torch.dot(r, r)
            if new_rdotr < 1e-10:
                break
            beta = new_rdotr / rdotr
            p = r + beta * p
            rdotr = new_rdotr
        return x

    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,
                              actor):  # 计算策略目标
        log_probs = torch.log(actor(states).gather(1, actions))
        ratio = torch.exp(log_probs - old_log_probs)
        return torch.mean(ratio * advantage)

    def line_search(self, states, actions, advantage, old_log_probs,
                    old_action_dists, max_vec):  # 线性搜索
        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(
            self.actor.parameters())
        old_obj = self.compute_surrogate_obj(states, actions, advantage,
                                             old_log_probs, self.actor)
        for i in range(15):  # 线性搜索主循环
            coef = self.alpha**i
            new_para = old_para + coef * max_vec
            new_actor = copy.deepcopy(self.actor)
            torch.nn.utils.convert_parameters.vector_to_parameters(
                new_para, new_actor.parameters())
            new_action_dists = torch.distributions.Categorical(
                new_actor(states))
            kl_div = torch.mean(
                torch.distributions.kl.kl_divergence(old_action_dists,
                                                     new_action_dists))
            new_obj = self.compute_surrogate_obj(states, actions, advantage,
                                                 old_log_probs, new_actor)
            if new_obj > old_obj and kl_div < self.kl_constraint:
                return new_para
        return old_para	#在自然梯度方向上搜索最大步长，确保新策略满足KL约束且目标函数提升。

    def policy_learn(self, states, actions, old_action_dists, old_log_probs,
                     advantage):  # 更新策略函数
        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,
                                                   old_log_probs, self.actor)	#用来计算替代优势
        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())
        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()	#得到目标梯度
        # 用共轭梯度法计算x = H^(-1)g
        descent_direction = self.conjugate_gradient(obj_grad, states,
                                                    old_action_dists) # 使用共轭梯度法计算自然策略梯度

        Hd = self.hessian_matrix_vector_product(states, old_action_dists,
                                                descent_direction)
        max_coef = torch.sqrt(2 * self.kl_constraint /
                              (torch.dot(descent_direction, Hd) + 1e-8))	#根据KL约束阈值计算最大允许步长。
        new_para = self.line_search(states, actions, advantage, old_log_probs,
                                    old_action_dists,
                                    descent_direction * max_coef)  # 线性搜索，确定最终参数
        torch.nn.utils.convert_parameters.vector_to_parameters(
            new_para, self.actor.parameters())  # 将线性搜索后的参数向量转换为策略网络的实际参数。

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        td_target = rewards + self.gamma * self.critic(next_states) * (1 -
                                                                       dones)
        td_delta = td_target - self.critic(states)	# 代表的是时序差分残差结果。
        advantage = compute_advantage(self.gamma, self.lmbda,
                                      td_delta.cpu()).to(self.device)	# 广义优势估计，用来估计优势函数A
        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()	#旧策略下动作的对数概率（用于重要性采样）。
        old_action_dists = torch.distributions.Categorical(
            self.actor(states).detach())	# 旧策略的动作分布（用于KL散度约束）。
        critic_loss = torch.mean(
            F.mse_loss(self.critic(states), td_target.detach()))	#最小化Critic网络预测值与TD目标的均方误差
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()  # 更新价值函数
        # 更新策略函数
        self.policy_learn(states, actions, old_action_dists, old_log_probs,
                          advantage)
```



## 4. 近端策略优化算法（PPO）

TRPO 算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。PPO 基于 TRPO 的思想，但是其算法实现更加简单。

TRPO的优化目标是

![image-20250420204351993](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420204351993.png)

PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断。

### PPO惩罚

![image-20250420204756880](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420204756880.png)

与自然策略梯度的推导很像，但是加入了$\beta$参数用于调节KL散度的调节

![image-20250420210519612](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420210519612.png)

$d_{targ}$是目标散度

### PPO截断 PPO-CLIP

CLIP在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，

![image-20250420205654087](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420205654087.png)

去掉KL散度约束项，将更新范围约束在$(1-\epsilon, 1+\epsilon)$范围内,超出限制就要进行截断

![image-20250420210641957](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250420210641957.png)

```python
import gym
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import rl_utils


class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)


class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)


class PPO:
    ''' PPO算法,采用截断方式 '''
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
                 lmbda, epochs, eps, gamma, device):
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)
        self.gamma = gamma
        self.lmbda = lmbda
        self.epochs = epochs  # 一条序列的数据用来训练轮数
        self.eps = eps  # PPO中截断范围的参数
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        td_target = rewards + self.gamma * self.critic(next_states) * (1 -
                                                                       dones)
        td_delta = td_target - self.critic(states)
        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,
                                               td_delta.cpu()).to(self.device)
        old_log_probs = torch.log(self.actor(states).gather(1,
                                                            actions)).detach()

        for _ in range(self.epochs):
            log_probs = torch.log(self.actor(states).gather(1, actions))
            ratio = torch.exp(log_probs - old_log_probs)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.eps,
                                1 + self.eps) * advantage  # 截断
            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数
            critic_loss = torch.mean(
                F.mse_loss(self.critic(states), td_target.detach()))
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            actor_loss.backward()
            critic_loss.backward()
            self.actor_optimizer.step()
            self.critic_optimizer.step()
```

## 5. PPO算法在LLM的应用

### 每个token的Reward怎么算？

![image-20250421092454874](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250421092454874.png)

### Reward-model的损失

![image-20250421092917503](C:\Users\DEL\AppData\Roaming\Typora\typora-user-images\image-20250421092917503.png)

每个训练样本包含chosen和rejected两个值，也就是这种形式{"question":xxxxx, "chosen":xxxxx, "rejected":xxxxx}
